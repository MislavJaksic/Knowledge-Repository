## Hive

### Installation

https://cwiki.apache.org/confluence/display/Hive/AdminManual+Installation

Download Hive. Unpack with "tar -xzvf hive-x.y.z.tar.gz".

Add to .bashrc:
´´´
export HIVE_HOME=/path/to/your/hive
export PATH=$HIVE_HOME/bin:$PATH
´´´
In addition, add to .bashrc:
´´´
HADOOP_HOME=/path/to/your/hadoop
´´´

Run Hadoop deamons, both start-dfs.sh and start-yarn.sh.
Execute the following commands:
´´´
$HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
$HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse
´´´
You can always check the HDFS with "$HADOOP_HOME/bin/hadoop fs -ls /".
If you cannot create a folder it means that you just have to construct the previous one first.

Try running "$HIVE_HOME/bin/hive".
If you get an error, see below.
´´´
<!-- Solution to the "... Relative path in absolute URI" problem -->
  <property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
  </property>
  <!-- ... solution ends -->
´´´

Instead of using HiveCLI which is depricated, you can use Beeline and HiveServer2.

https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2

The following is already configured:
´´´
hive.server2.thrift.min.worker.threads – Minimum number of worker threads, default 5.
hive.server2.thrift.max.worker.threads – Maximum number of worker threads, default 500.
hive.server2.thrift.port – TCP port number to listen on, default 10000.
hive.server2.thrift.bind.host – TCP interface to bind to, NOT SET.

hive.server2.transport.mode - Set to http to enable HTTP transport mode, default binary.
hive.server2.thrift.http.port - HTTP port number to listen on, default 10001.
hive.server2.thrift.http.max.worker.threads - Maximum worker threads in the server pool, default 500.
hive.server2.thrift.http.min.worker.threads - Minimum worker threads in the server pool, default 5.
hive.server2.thrift.http.path - The service endpoint, default cliservice.
´´´

The server can run in either HTTP or TCP mode, not both.
Start HiveServer2 using "bin/hiveserver2". If everything starts correctly, there will be no prompts.

Start Beeline with "bin/beeline".
Execute "!connect jdbc:hive2://localhost:10000". If connecting remotely, use "bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT".
Test the connection with "!tables". If it works, you are in!


https://cwiki.apache.org/confluence/display/Hive/HiveDerbyServerMode

Install and configure Apache Derby.
TODO... Installing Derby.


https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration
TODO... Fine tune the config.


### (Unofficial) installation

https://www.tutorialspoint.com/hive/hive_installation.htm

You need to have both Java and Hadoop installed.

Download Hive. Unpack it with "tar -xzvf apache-hive-x.y.z-bin.tar.gz".

Add to .bashrc.
´´´
export HIVE_HOME=/path/to/your/hive !!
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/path/to/your/hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/path/to/your/hive/lib/*:.
´´´

source ~/.bashrc - refresh the file

Check paths and create a config file with:
cd $HIVE_HOME/conf
cp hive-env.sh.template hive-env.sh


In the following files append the following:

* hive.env.sh
´´´
export HADOOP_HOME=/path/to/your/hadoop
´´´

TODO Stopped at Step 8: Verifying Hive Installation ...


### Test installation

Don't forget to run start-dfs.sh, also known as Hadoop, first.

Test using bin/hive in the /path/to/your/hive folder.


### Hive Streaming

https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest
https://community.hortonworks.com/articles/49949/test-7.html

hive-site.xml

hive.txn.manager                = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
hive.compactor.initiator.on     = true
hive.compactor.worker.threads   = 1 (greater then 0)

hive.support.concurrency        = true
hive.enforce.bucketing          = true
hive.exec.dynamic.partition.mode= nonstrict
hive.txn.timeout                = 300

Table has to be "store as orc" and "transactional"="true" and must be bucketed but not sorted and users need to have special permissions.
Example:
 CREATE TABLE alarms (stamp TIMESTAMP, state STRING, status_code STRING, down TINYINT, type STRING, info STRING)
 CLUSTERED BY(stamp) INTO 10 BUCKETS
 STORED AS ORC
 tblproperties("transactional"="true");


### Hive test



After starting Hadoop with "start-dfs.sh" and "start-yarn.sh" and HiveServer2 with "bin/hiveserver2" do the following:

start Beeline with "bin/beeline",

execute "!connect jdbc:hive2://localhost:10000" or "bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT" if connecting remotely,

execute "!tables" and "!dbinfo".
